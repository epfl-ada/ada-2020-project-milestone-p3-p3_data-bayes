{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 1. Data reading and preprocessing\n",
    " "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1.1 imports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/cormak/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:17: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.9 it will stop working\n",
      "  from collections import Mapping, defaultdict\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: en_core_web_sm==2.3.1 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz#egg=en_core_web_sm==2.3.1 in /home/cormak/.local/lib/python3.8/site-packages (2.3.1)\n",
      "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in /home/cormak/.local/lib/python3.8/site-packages (from en_core_web_sm==2.3.1) (2.3.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/cormak/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.54.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/cormak/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /home/cormak/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.4)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /home/cormak/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.3)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (50.3.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/cormak/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.19.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.24.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/cormak/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.4)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/cormak/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0; python_version >= \"3.6\" in /home/cormak/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.7.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/cormak/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.8.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/cormak/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.4)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/cormak/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n",
      "Requirement already satisfied: chardet>=3.0.2 in /usr/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\n",
      "Requirement already satisfied: idna>=2.5 in /usr/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.10)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in /usr/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.25.10)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/home/cormak/.local/lib/python3.8/site-packages/en_core_web_sm -->\n",
      "/home/cormak/.local/lib/python3.8/site-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already up-to-date: scikit-learn==0.19.2 in /home/cormak/.local/lib/python3.8/site-packages (0.19.2)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import convokit\n",
    "from convokit.politenessStrategies.politenessStrategies import PolitenessStrategies\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "!python -m spacy download en\n",
    "from convokit import Corpus, download\n",
    "import pickle\n",
    "import sklearn\n",
    "#from convokit.classifier.classifier import Classifier\n",
    "from convokit import TextParser\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "from spacy.lang.en import English\n",
    "!pip3 install -U scikit-learn==0.19.2\n",
    "import politeness\n",
    "import joblib"
   ]
  },
  {
   "source": [
    "## 1.2 politeness"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "* I decided to take the annotated wiki corpus to train our politeness classifier as I could notfind a working pre trained model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset already exists at /home/cormak/.convokit/downloads/wiki-politeness-annotated\n"
     ]
    }
   ],
   "source": [
    "parser = TextParser(verbosity=1000)\n",
    "train_corpus = Corpus(filename=download('wiki-politeness-annotated'))\n",
    "#parser.transform(train_corpus)\n",
    "ps = PolitenessStrategies()\n",
    "#ps.transform(train_corpus)\n",
    "#clf = Classifier(obj_type='utterance', pred_feats=['politeness_strategies'], \n",
    "#                 labeller=lambda utt: utt.meta['Binary']==1)\n",
    "#clf.fit(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset already exists at /home/cormak/.convokit/downloads/diplomacy-corpus\n",
      "1000/17289 utterances processed\n",
      "2000/17289 utterances processed\n",
      "3000/17289 utterances processed\n",
      "4000/17289 utterances processed\n",
      "5000/17289 utterances processed\n",
      "6000/17289 utterances processed\n",
      "7000/17289 utterances processed\n",
      "8000/17289 utterances processed\n",
      "9000/17289 utterances processed\n",
      "10000/17289 utterances processed\n",
      "11000/17289 utterances processed\n",
      "12000/17289 utterances processed\n",
      "13000/17289 utterances processed\n",
      "14000/17289 utterances processed\n",
      "15000/17289 utterances processed\n",
      "16000/17289 utterances processed\n",
      "17000/17289 utterances processed\n",
      "17289/17289 utterances processed\n"
     ]
    }
   ],
   "source": [
    "deception = Corpus(filename=download(\"diplomacy-corpus\"))\n",
    "deception=parser.transform(deception)\n",
    "deception=ps.transform(deception)\n"
   ]
  },
  {
   "source": [
    "* creation of features for politeness"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[nltk_data] Downloading package punkt to /home/cormak/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n{'feature_politeness_==Please==': 0, 'feature_politeness_==Please_start==': 0, 'feature_politeness_==HASHEDGE==': 1, 'feature_politeness_==Indirect_(btw)==': 0, 'feature_politeness_==Hedges==': 1, 'feature_politeness_==Factuality==': 0, 'feature_politeness_==Deference==': 0, 'feature_politeness_==Gratitude==': 0, 'feature_politeness_==Apologizing==': 0, 'feature_politeness_==1st_person_pl.==': 0, 'feature_politeness_==1st_person==': 0, 'feature_politeness_==1st_person_start==': 1, 'feature_politeness_==2nd_person==': 1, 'feature_politeness_==2nd_person_start==': 0, 'feature_politeness_==Indirect_(greeting)==': 0, 'feature_politeness_==Direct_question==': 0, 'feature_politeness_==Direct_start==': 0, 'feature_politeness_==HASPOSITIVE==': 0, 'feature_politeness_==HASNEGATIVE==': 0, 'feature_politeness_==SUBJUNCTIVE==': 0, 'feature_politeness_==INDICATIVE==': 0}\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "X.shape[1] = 21 should be equal to 1447, the number of features at training time",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-9c7cb8011b1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_utterance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-9c7cb8011b1b>\u001b[0m in \u001b[0;36mscore\u001b[0;34m(utterance)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Single-row sparse matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Massage return format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"polite\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"impolite\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_predict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_predict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_for_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobA_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobB_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m             raise NotFittedError(\"predict_proba is not available when fitted \"\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_validate_for_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    455\u001b[0m                                  (X.shape[1], self.shape_fit_[0]))\n\u001b[1;32m    456\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape_fit_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m             raise ValueError(\"X.shape[1] = %d should be equal to %d, \"\n\u001b[0m\u001b[1;32m    458\u001b[0m                              \u001b[0;34m\"the number of features at training time\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m                              (n_features, self.shape_fit_[1]))\n",
      "\u001b[0;31mValueError\u001b[0m: X.shape[1] = 21 should be equal to 1447, the number of features at training time"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "import _pickle\n",
    "nltk.download('punkt')\n",
    "MODEL_FILENAME='./politeness/politeness-svm.p'\n",
    "clf = _pickle.load(open(MODEL_FILENAME, 'rb'), encoding='latin1', fix_imports=True)\n",
    "\n",
    "\n",
    "def score(utterance):\n",
    "    # Vectorizer returns {feature-name: value} dict\n",
    "    features = utterance.meta['politeness_strategies']\n",
    "    print(features)\n",
    "    fv = [features[f] for f in sorted(features.keys())]\n",
    "    # Single-row sparse matrix\n",
    "    X = csr_matrix(np.asarray([fv]))\n",
    "    probs = clf.predict_proba(X)\n",
    "    # Massage return format\n",
    "    probs = {\"polite\": probs[0][1], \"impolite\": probs[0][0]}\n",
    "    return probs\n",
    "\n",
    "probs = score(deception.random_utterance())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[nltk_data] Downloading package punkt to /home/cormak/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n{'sentences': ['Have you found the answer for your question?', 'If yes would you please share it?'], 'parses': [['csubj(found-3, Have-1)', 'dobj(Have-1, you-2)', 'root(ROOT-0, found-3)', 'det(answer-5, the-4)', 'dobj(found-3, answer-5)', 'poss(question-8, your-7)', 'prep_for(found-3, question-8)'], ['prep_if(would-3, yes-2)', 'root(ROOT-0, would-3)', 'nsubj(would-3, you-4)', 'ccomp(would-3, please-5)', 'nsubj(it-7, share-6)', 'xcomp(please-5, it-7)']]}\n{'feature_politeness_==Please==': 1, 'feature_politeness_==Please_start==': 0, 'feature_politeness_==Indirect_(btw)==': 0, 'feature_politeness_==Hedges==': 0, 'feature_politeness_==Factuality==': 0, 'feature_politeness_==Deference==': 0, 'feature_politeness_==Gratitude==': 0, 'feature_politeness_==Apologizing==': 0, 'feature_politeness_==1st_person_pl.==': 0, 'feature_politeness_==1st_person==': 0, 'feature_politeness_==1st_person_start==': 0, 'feature_politeness_==2nd_person==': 1, 'feature_politeness_==2nd_person_start==': 0, 'feature_politeness_==Indirect_(greeting)==': 0, 'feature_politeness_==Direct_question==': 0, 'feature_politeness_==Direct_start==': 0, 'feature_politeness_==SUBJUNCTIVE==': 1, 'feature_politeness_==INDICATIVE==': 0, 'feature_politeness_==HASHEDGE==': 0, 'feature_politeness_==HASPOSITIVE==': 0, 'feature_politeness_==HASNEGATIVE==': 0}\n[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "X.shape[1] = 21 should be equal to 1447, the number of features at training time",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-da153727e82b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m score({\n\u001b[0m\u001b[1;32m     59\u001b[0m             'sentences': [\n\u001b[1;32m     60\u001b[0m                 \u001b[0;34m\"Have you found the answer for your question?\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-da153727e82b>\u001b[0m in \u001b[0;36mscore\u001b[0;34m(request)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m# Single-row sparse matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0;31m# Massage return format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"polite\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"impolite\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_predict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_predict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_for_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobA_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobB_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m             raise NotFittedError(\"predict_proba is not available when fitted \"\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_validate_for_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    455\u001b[0m                                  (X.shape[1], self.shape_fit_[0]))\n\u001b[1;32m    456\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape_fit_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m             raise ValueError(\"X.shape[1] = %d should be equal to %d, \"\n\u001b[0m\u001b[1;32m    458\u001b[0m                              \u001b[0;34m\"the number of features at training time\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m                              (n_features, self.shape_fit_[1]))\n",
      "\u001b[0;31mValueError\u001b[0m: X.shape[1] = 21 should be equal to 1447, the number of features at training time"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "packages2versions = [(\"scikit-learn\", sklearn, \"0.18.1\"),\n",
    "                     (\"numpy\", np, \"1.12.0\"),\n",
    "                     (\"nltk\", nltk, \"3.2.1\"),\n",
    "                     (\"scipy\", scipy, \"0.18.1\")]\n",
    "\n",
    "for name, package, expected_v in packages2versions:\n",
    "    if package.__version__ < expected_v:\n",
    "        sys.stderr.write(\"Warning: package '%s', expected version >= %s, detec\",\n",
    "                         \"ted %s. Code functionality not guaranteed.\\n\"\n",
    "                         % (name, expected_v, package.__version__))\n",
    "\n",
    "from politeness.features.vectorizer import PolitenessFeatureVectorizer\n",
    "vectorizer = PolitenessFeatureVectorizer()\n",
    "\n",
    "import _pickle\n",
    "nltk.download('punkt')\n",
    "\n",
    "def score(request):\n",
    "    \"\"\"\n",
    "    :param request - The request document to score\n",
    "    :type request - dict with 'sentences' and 'parses' field\n",
    "        sample (taken from test_documents.py)--\n",
    "        {\n",
    "            'sentences': [\n",
    "                \"Have you found the answer for your question?\",\n",
    "                \"If yes would you please share it?\"\n",
    "            ],\n",
    "            'parses': [\n",
    "                [\"csubj(found-3, Have-1)\", \"dobj(Have-1, you-2)\",\n",
    "                 \"root(ROOT-0, found-3)\", \"det(answer-5, the-4)\",\n",
    "                 \"dobj(found-3, answer-5)\", \"poss(question-8, your-7)\",\n",
    "                 \"prep_for(found-3, question-8)\"],\n",
    "                [\"prep_if(would-3, yes-2)\", \"root(ROOT-0, would-3)\",\n",
    "                 \"nsubj(would-3, you-4)\", \"ccomp(would-3, please-5)\",\n",
    "                 \"nsubj(it-7, share-6)\", \"xcomp(please-5, it-7)\"]\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    returns class probabilities as a dict\n",
    "        { 'polite': float, 'impolite': float }\n",
    "    \"\"\"\n",
    "    # Vectorizer returns {feature-name: value} dict\n",
    "    print(request)\n",
    "    features = vectorizer.features(request)\n",
    "    print(features)\n",
    "    fv = [features[f] for f in sorted(features.keys())]\n",
    "    print(fv)\n",
    "    # Single-row sparse matrix\n",
    "    X = csr_matrix(np.asarray([fv]))\n",
    "    probs = clf.predict_proba(X)\n",
    "    # Massage return format\n",
    "    probs = {\"polite\": probs[0][1], \"impolite\": probs[0][0]}\n",
    "    return probs\n",
    "\n",
    "\n",
    "score({\n",
    "            'sentences': [\n",
    "                \"Have you found the answer for your question?\",\n",
    "                \"If yes would you please share it?\"\n",
    "            ],\n",
    "            'parses': [\n",
    "                [\"csubj(found-3, Have-1)\", \"dobj(Have-1, you-2)\",\n",
    "                 \"root(ROOT-0, found-3)\", \"det(answer-5, the-4)\",\n",
    "                 \"dobj(found-3, answer-5)\", \"poss(question-8, your-7)\",\n",
    "                 \"prep_for(found-3, question-8)\"],\n",
    "                [\"prep_if(would-3, yes-2)\", \"root(ROOT-0, would-3)\",\n",
    "                 \"nsubj(would-3, you-4)\", \"ccomp(would-3, please-5)\",\n",
    "                 \"nsubj(it-7, share-6)\", \"xcomp(please-5, it-7)\"]\n",
    "            ]\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'SVC' object has no attribute 'transform_objs'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-8ed9dfb0a421>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_utterance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_objs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'SVC' object has no attribute 'transform_objs'"
     ]
    }
   ],
   "source": [
    "\n",
    "sentence=parser.transform_utterance(\"Have you found the answer for your question?\")\n",
    "sentence=ps.transform_utterance(sentence)\n",
    "\n",
    "clf.transform_objs([sentence])[0].meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1000/17289 utterances processed\n",
      "2000/17289 utterances processed\n",
      "3000/17289 utterances processed\n",
      "4000/17289 utterances processed\n",
      "5000/17289 utterances processed\n",
      "6000/17289 utterances processed\n",
      "7000/17289 utterances processed\n",
      "8000/17289 utterances processed\n",
      "9000/17289 utterances processed\n",
      "10000/17289 utterances processed\n",
      "11000/17289 utterances processed\n",
      "12000/17289 utterances processed\n",
      "13000/17289 utterances processed\n",
      "14000/17289 utterances processed\n",
      "15000/17289 utterances processed\n",
      "16000/17289 utterances processed\n",
      "17000/17289 utterances processed\n",
      "17289/17289 utterances processed\n"
     ]
    }
   ],
   "source": [
    "{\n",
    "    'sentences': [\n",
    "        \"Have you found the answer for your question?\",\n",
    "        \"If yes would you please share it?\"\n",
    "    ],\n",
    "    'parses': [\n",
    "        [\"csubj(found-3, Have-1)\", \"dobj(Have-1, you-2)\",\n",
    "            \"root(ROOT-0, found-3)\", \"det(answer-5, the-4)\",\n",
    "            \"dobj(found-3, answer-5)\", \"poss(question-8, your-7)\",\n",
    "            \"prep_for(found-3, question-8)\"],\n",
    "        [\"prep_if(would-3, yes-2)\", \"root(ROOT-0, would-3)\",\n",
    "            \"nsubj(would-3, you-4)\", \"ccomp(would-3, please-5)\",\n",
    "            \"nsubj(it-7, share-6)\", \"xcomp(please-5, it-7)\"]\n",
    "    ]\n",
    "}\n",
    "deception = parser.transform(deception)\n",
    "deception = ps.transform(deception, markers=True)\n",
    "summary = ps.summarize(deception)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Utterance({'obj_type': 'utterance', 'meta': {'parsed': [{'rt': 2, 'toks': [{'tok': 'Have', 'tag': 'VBP', 'dep': 'aux', 'up': 2, 'dn': []}, {'tok': 'you', 'tag': 'PRP', 'dep': 'nsubj', 'up': 2, 'dn': []}, {'tok': 'found', 'tag': 'VBN', 'dep': 'ROOT', 'dn': [0, 1, 4, 8]}, {'tok': 'the', 'tag': 'DT', 'dep': 'det', 'up': 4, 'dn': []}, {'tok': 'answer', 'tag': 'NN', 'dep': 'dobj', 'up': 2, 'dn': [3, 5]}, {'tok': 'for', 'tag': 'IN', 'dep': 'prep', 'up': 4, 'dn': [7]}, {'tok': 'your', 'tag': 'PRP$', 'dep': 'poss', 'up': 7, 'dn': []}, {'tok': 'question', 'tag': 'NN', 'dep': 'pobj', 'up': 5, 'dn': [6]}, {'tok': '?', 'tag': '.', 'dep': 'punct', 'up': 2, 'dn': []}]}]}, 'vectors': [], 'speaker': None, 'conversation_id': None, 'reply_to': None, 'timestamp': None, 'text': 'Have you found the answer for your question?', 'owner': None, 'id': None})"
      ]
     },
     "metadata": {},
     "execution_count": 74
    }
   ],
   "source": [
    "parser.transform_utterance(\"Have you found the answer for your question?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2020-12-04 23:25:44 WARNING: Directory /home/cormak/stanza_corenlp already exists. Please install CoreNLP to a new directory.\n",
      "2020-12-04 23:25:44 INFO: Writing properties to tmp file: corenlp_server-a16f973794894b5b.props\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# test stanza\n",
    "from stanza.server import CoreNLPClient\n",
    "import stanza as stanza\n",
    "stanza.install_corenlp()\n",
    "client = CoreNLPClient(\n",
    "        annotators=['tokenize','ssplit','pos', 'lemma','ner',  'depparse','coref', 'kbp','tokenize','ssplit','pos'],\n",
    "        timeout=30000,\n",
    "        memory='16G', be_quiet=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2020-12-04 23:23:39 INFO: Starting server with command: java -Xmx16G -cp /home/cormak/stanza_corenlp/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 30000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-a6babde155a04ceb.props -annotators tokenize,ssplit,pos,lemma,ner,parse,depparse,coref -preload -outputFormat serialized\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "child {\n",
       "  child {\n",
       "    child {\n",
       "      value: \"Have\"\n",
       "    }\n",
       "    value: \"VBP\"\n",
       "    score: -0.62845379114151\n",
       "  }\n",
       "  child {\n",
       "    child {\n",
       "      child {\n",
       "        value: \"you\"\n",
       "      }\n",
       "      value: \"PRP\"\n",
       "      score: -2.1667263507843018\n",
       "    }\n",
       "    value: \"NP\"\n",
       "    score: -2.8400943279266357\n",
       "  }\n",
       "  child {\n",
       "    child {\n",
       "      child {\n",
       "        value: \"found\"\n",
       "      }\n",
       "      value: \"VBN\"\n",
       "      score: -4.834004878997803\n",
       "    }\n",
       "    child {\n",
       "      child {\n",
       "        child {\n",
       "          child {\n",
       "            value: \"the\"\n",
       "          }\n",
       "          value: \"DT\"\n",
       "          score: -0.5893322825431824\n",
       "        }\n",
       "        child {\n",
       "          child {\n",
       "            value: \"answer\"\n",
       "          }\n",
       "          value: \"NN\"\n",
       "          score: -8.24889850616455\n",
       "        }\n",
       "        value: \"NP\"\n",
       "        score: -10.436527252197266\n",
       "      }\n",
       "      child {\n",
       "        child {\n",
       "          child {\n",
       "            value: \"for\"\n",
       "          }\n",
       "          value: \"IN\"\n",
       "          score: -2.6398026943206787\n",
       "        }\n",
       "        child {\n",
       "          child {\n",
       "            child {\n",
       "              value: \"your\"\n",
       "            }\n",
       "            value: \"PRP$\"\n",
       "            score: -2.4493396282196045\n",
       "          }\n",
       "          child {\n",
       "            child {\n",
       "              value: \"question\"\n",
       "            }\n",
       "            value: \"NN\"\n",
       "            score: -6.845937252044678\n",
       "          }\n",
       "          value: \"NP\"\n",
       "          score: -13.15860652923584\n",
       "        }\n",
       "        value: \"PP\"\n",
       "        score: -16.21947479248047\n",
       "      }\n",
       "      value: \"NP\"\n",
       "      score: -27.078540802001953\n",
       "    }\n",
       "    value: \"VP\"\n",
       "    score: -35.86427307128906\n",
       "  }\n",
       "  child {\n",
       "    child {\n",
       "      value: \"?\"\n",
       "    }\n",
       "    value: \".\"\n",
       "    score: -0.27232077717781067\n",
       "  }\n",
       "  value: \"SQ\"\n",
       "  score: -43.65590286254883\n",
       "}\n",
       "value: \"ROOT\"\n",
       "score: -48.14925003051758"
      ]
     },
     "metadata": {},
     "execution_count": 118
    }
   ],
   "source": [
    "ann = client.annotate(\n",
    "\"Have you found the answer for your question? If yes would you please share it?\"\n",
    ")\n",
    "sentence = ann.sentence[0]\n",
    "sentence.parseTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "node {\n  sentenceIndex: 0\n  index: 1\n}\nnode {\n  sentenceIndex: 0\n  index: 2\n}\nnode {\n  sentenceIndex: 0\n  index: 3\n}\nnode {\n  sentenceIndex: 0\n  index: 4\n}\nnode {\n  sentenceIndex: 0\n  index: 5\n}\nnode {\n  sentenceIndex: 0\n  index: 6\n}\nnode {\n  sentenceIndex: 0\n  index: 7\n}\nnode {\n  sentenceIndex: 0\n  index: 8\n}\nnode {\n  sentenceIndex: 0\n  index: 9\n}\nedge {\n  source: 3\n  target: 1\n  dep: \"aux\"\n  isExtra: false\n  sourceCopy: 0\n  targetCopy: 0\n  language: UniversalEnglish\n}\nedge {\n  source: 3\n  target: 2\n  dep: \"nsubj\"\n  isExtra: false\n  sourceCopy: 0\n  targetCopy: 0\n  language: UniversalEnglish\n}\nedge {\n  source: 3\n  target: 5\n  dep: \"obj\"\n  isExtra: false\n  sourceCopy: 0\n  targetCopy: 0\n  language: UniversalEnglish\n}\nedge {\n  source: 3\n  target: 9\n  dep: \"punct\"\n  isExtra: false\n  sourceCopy: 0\n  targetCopy: 0\n  language: UniversalEnglish\n}\nedge {\n  source: 5\n  target: 4\n  dep: \"det\"\n  isExtra: false\n  sourceCopy: 0\n  targetCopy: 0\n  language: UniversalEnglish\n}\nedge {\n  source: 5\n  target: 8\n  dep: \"nmod\"\n  isExtra: false\n  sourceCopy: 0\n  targetCopy: 0\n  language: UniversalEnglish\n}\nedge {\n  source: 8\n  target: 6\n  dep: \"case\"\n  isExtra: false\n  sourceCopy: 0\n  targetCopy: 0\n  language: UniversalEnglish\n}\nedge {\n  source: 8\n  target: 7\n  dep: \"nmod:poss\"\n  isExtra: false\n  sourceCopy: 0\n  targetCopy: 0\n  language: UniversalEnglish\n}\nroot: 3\n\n"
     ]
    }
   ],
   "source": [
    "print(sentence.basicDependencies)\n"
   ]
  }
 ]
}