{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import pycountry\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('deception_df.pkl')\n",
    "df_2 = pd.read_json('diplomacy_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                messages  sender_labels  \\\n",
       "5606             I managed to piss Germany off for you ðŸ˜‰           True   \n",
       "1350   Oh fair, because you're not actually moving in...           True   \n",
       "13187  You stabbing the French is already a massive l...           True   \n",
       "\n",
       "      receiver_labels speakers receivers  absolute_message_index  \\\n",
       "5606             True  austria   england                    2675   \n",
       "1350             True  england   germany                     407   \n",
       "13187            True    italy   germany                    1007   \n",
       "\n",
       "       relative_message_index seasons years game_score game_score_delta  \\\n",
       "5606                       95  Spring  1905          8                2   \n",
       "1350                       61  Spring  1902          4               -1   \n",
       "13187                      55  Winter  1904          3               -6   \n",
       "\n",
       "               players  game_id  politeness  negative_sentiment  \\\n",
       "5606   austria,england        2    2.220411                 1.0   \n",
       "1350   germany,england        1    3.327725                 0.4   \n",
       "13187    italy,germany       11    3.150316                 0.5   \n",
       "\n",
       "       neutral_sentiment  positive_sentiment  vader_score  \n",
       "5606                 0.0                 0.0      -0.4019  \n",
       "1350                 0.6                 0.0       0.4019  \n",
       "13187                0.5                 0.0      -0.3412  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>messages</th>\n      <th>sender_labels</th>\n      <th>receiver_labels</th>\n      <th>speakers</th>\n      <th>receivers</th>\n      <th>absolute_message_index</th>\n      <th>relative_message_index</th>\n      <th>seasons</th>\n      <th>years</th>\n      <th>game_score</th>\n      <th>game_score_delta</th>\n      <th>players</th>\n      <th>game_id</th>\n      <th>politeness</th>\n      <th>negative_sentiment</th>\n      <th>neutral_sentiment</th>\n      <th>positive_sentiment</th>\n      <th>vader_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>5606</th>\n      <td>I managed to piss Germany off for you ðŸ˜‰</td>\n      <td>True</td>\n      <td>True</td>\n      <td>austria</td>\n      <td>england</td>\n      <td>2675</td>\n      <td>95</td>\n      <td>Spring</td>\n      <td>1905</td>\n      <td>8</td>\n      <td>2</td>\n      <td>austria,england</td>\n      <td>2</td>\n      <td>2.220411</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.4019</td>\n    </tr>\n    <tr>\n      <th>1350</th>\n      <td>Oh fair, because you're not actually moving in...</td>\n      <td>True</td>\n      <td>True</td>\n      <td>england</td>\n      <td>germany</td>\n      <td>407</td>\n      <td>61</td>\n      <td>Spring</td>\n      <td>1902</td>\n      <td>4</td>\n      <td>-1</td>\n      <td>germany,england</td>\n      <td>1</td>\n      <td>3.327725</td>\n      <td>0.4</td>\n      <td>0.6</td>\n      <td>0.0</td>\n      <td>0.4019</td>\n    </tr>\n    <tr>\n      <th>13187</th>\n      <td>You stabbing the French is already a massive l...</td>\n      <td>True</td>\n      <td>True</td>\n      <td>italy</td>\n      <td>germany</td>\n      <td>1007</td>\n      <td>55</td>\n      <td>Winter</td>\n      <td>1904</td>\n      <td>3</td>\n      <td>-6</td>\n      <td>italy,germany</td>\n      <td>11</td>\n      <td>3.150316</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>0.0</td>\n      <td>-0.3412</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 129
    }
   ],
   "source": [
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       absolute_message_index  relative_message_index       game_id  \\\n",
       "count            17289.000000            17289.000000  17289.000000   \n",
       "mean               948.323153              113.711782      5.401990   \n",
       "std                750.710891              129.367961      3.778135   \n",
       "min                  0.000000                0.000000      1.000000   \n",
       "25%                360.000000               22.000000      2.000000   \n",
       "50%                769.000000               65.000000      4.000000   \n",
       "75%               1347.000000              152.000000      9.000000   \n",
       "max               3301.000000              674.000000     12.000000   \n",
       "\n",
       "         politeness  negative_sentiment  neutral_sentiment  \\\n",
       "count  17289.000000        17289.000000       17289.000000   \n",
       "mean       3.335990            0.254738           0.605296   \n",
       "std        0.333862            0.376942           0.420526   \n",
       "min        1.349828            0.000000           0.000000   \n",
       "25%        3.168759            0.000000           0.000000   \n",
       "50%        3.297227            0.000000           0.666667   \n",
       "75%        3.461842            0.500000           1.000000   \n",
       "max        4.588427            1.000000           1.000000   \n",
       "\n",
       "       positive_sentiment   vader_score  \n",
       "count        17289.000000  17289.000000  \n",
       "mean             0.139966      0.195610  \n",
       "std              0.303795      0.413835  \n",
       "min              0.000000     -0.973100  \n",
       "25%              0.000000      0.000000  \n",
       "50%              0.000000      0.168000  \n",
       "75%              0.000000      0.510600  \n",
       "max              1.000000      0.991600  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>absolute_message_index</th>\n      <th>relative_message_index</th>\n      <th>game_id</th>\n      <th>politeness</th>\n      <th>negative_sentiment</th>\n      <th>neutral_sentiment</th>\n      <th>positive_sentiment</th>\n      <th>vader_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>17289.000000</td>\n      <td>17289.000000</td>\n      <td>17289.000000</td>\n      <td>17289.000000</td>\n      <td>17289.000000</td>\n      <td>17289.000000</td>\n      <td>17289.000000</td>\n      <td>17289.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>948.323153</td>\n      <td>113.711782</td>\n      <td>5.401990</td>\n      <td>3.335990</td>\n      <td>0.254738</td>\n      <td>0.605296</td>\n      <td>0.139966</td>\n      <td>0.195610</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>750.710891</td>\n      <td>129.367961</td>\n      <td>3.778135</td>\n      <td>0.333862</td>\n      <td>0.376942</td>\n      <td>0.420526</td>\n      <td>0.303795</td>\n      <td>0.413835</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>1.349828</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-0.973100</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>360.000000</td>\n      <td>22.000000</td>\n      <td>2.000000</td>\n      <td>3.168759</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>769.000000</td>\n      <td>65.000000</td>\n      <td>4.000000</td>\n      <td>3.297227</td>\n      <td>0.000000</td>\n      <td>0.666667</td>\n      <td>0.000000</td>\n      <td>0.168000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>1347.000000</td>\n      <td>152.000000</td>\n      <td>9.000000</td>\n      <td>3.461842</td>\n      <td>0.500000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.510600</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>3301.000000</td>\n      <td>674.000000</td>\n      <td>12.000000</td>\n      <td>4.588427</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.991600</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 130
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "source": [
    "### 1. Looking at some messages for some politeness score slices (start, first quartile, third quartile, end)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "* #### Looking at least polite messages"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Score: 1.350 | Message: I will not fuck you over\nScore: 1.422 | Message: What a clod you are. How in blue fuck does keeping Turkey in the game benefit you in the least.\nScore: 1.449 | Message: Donâ€™t fuck me over tho\nScore: 1.468 | Message: You're a dick dude.  I will make sure that that's the last one you take.  I'll let E/G walk over me of I have to\nScore: 1.477 | Message: I was getting you into Kiel. You made a bad decision, you ruined my game, and you can go fuck yourself.\n"
     ]
    }
   ],
   "source": [
    "for _, row in df.sort_values('politeness').head(5).iterrows():\n",
    "    print(f\"Score: {row['politeness']:.3f} | Message: {row['messages']}\")"
   ]
  },
  {
   "source": [
    "* #### Looking at first politeness quartile messages"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Score: 3.169 | Message: Totally understand that, I donâ€™t want to be allied with the Turks but warring them and Russia at the same time is a pain in the ass. Iâ€™m convinced that they are against Russia and will help me until they know russia will go down (which could very well only be this fall or fall and spring)\nScore: 3.169 | Message: Heâ€™s a little salty with me for the Belgium thing\nScore: 3.169 | Message: Okay Iâ€™m reading your proposal again and are you saying you would get all of russia?\nScore: 3.169 | Message: If France is honest with me, Ruhr will be free and Bur will be occupied. \n\nLet's be real here, though: Russia has at most two units to spare to threaten you, and likely not even that given the imminent pressure from Austria. I'm trying to work it out in the sandbox now but I really don't think they'd be an issue for you...\nScore: 3.169 | Message: Either I build armies and funnel through Scandinavia to help you or build fleets and take on Italy.\n"
     ]
    }
   ],
   "source": [
    "for _, row in df.sort_values('politeness').iloc[int(len(df)/4):,:].head(5).iterrows():\n",
    "    print(f\"Score: {row['politeness']:.3f} | Message: {row['messages']}\")"
   ]
  },
  {
   "source": [
    "* #### Looking at last politeness quartile messages"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Score: 3.462 | Message: Yeah, I think you'll be fine with Austria initially\nScore: 3.462 | Message: You are correct, and that's why I want the army to go down there to help out silly head. Not sure where the confusion stems from.\nScore: 3.462 | Message: Yeah I was thinking actually that it would make sense to put Russia into munich\nScore: 3.462 | Message: Hey Italy, everything well over here. We probably won't be directly involved with each-other until mid-game, but if we're both still around, a France/Italy alliance could be pretty strongðŸ˜‰ .\nScore: 3.462 | Message: Yeah prob either south coast or army. Im thinking an army could be good since I already have one fleet there\n"
     ]
    }
   ],
   "source": [
    "for _, row in df.sort_values('politeness').iloc[int(3*len(df)/4):,:].head(5).iterrows():\n",
    "    print(f\"Score: {row['politeness']:.3f} | Message: {row['messages']}\")"
   ]
  },
  {
   "source": [
    "* #### Looking at most polite messages"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Score: 4.588 | Message: Thanks, much appreciated - Hoping to work together more going forward!\nScore: 4.584 | Message: Ok, thank you!\nScore: 4.567 | Message: That would be great! Thanks!\nScore: 4.566 | Message: Okay, thank you for letting me know. That helps!\nScore: 4.554 | Message: Sounds great man! Thanks for everything!\n"
     ]
    }
   ],
   "source": [
    "for _, row in df.sort_values('politeness', ascending=False).head(5).iterrows():\n",
    "    print(f\"Score: {row['politeness']:.3f} | Message: {row['messages']}\")"
   ]
  },
  {
   "source": [
    "### 2. Looking at term frequency for different politeness slices "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "* #### least polite messages"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "we take only 100 messages here to really be able to study extremely unpolite messages"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "      Word  Frequency\n0       im         29\n1     fuck         27\n2  fucking         15\n3     shit         14\n4     game         11\n5   fuckin          9\n6    going          9\n7     dont          8\n8      two          8\n9     army          8",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Word</th>\n      <th>Frequency</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>im</td>\n      <td>29</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>fuck</td>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>fucking</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>shit</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>game</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>fuckin</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>going</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>dont</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>two</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>army</td>\n      <td>8</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# check for least polite vocabulary  \n",
    "\n",
    "\n",
    "top_N = 10\n",
    "txt = df.sort_values('politeness').head(100).messages.str.lower().str.replace(r'\\|', ' ').str.cat(sep=' ')\n",
    "txt = txt.translate(str.maketrans('', '', string.punctuation+'â€™Â´'))\n",
    "\n",
    "\n",
    "pattern = re.compile(names, re.UNICODE | re.IGNORECASE)\n",
    "words = nltk.tokenize.word_tokenize(txt)\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "words_dist = nltk.FreqDist(w for w in words if w not in stopwords+['england','russia','france','turkey','germany','italy','austria-hungary', 'austria'])\n",
    "\n",
    "rslt = pd.DataFrame(words_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "display(rslt)"
   ]
  },
  {
   "source": [
    "* #### beginning of first quartile vocabulary"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "studying the quartiles we are not in the extremes anymore and can therefore take more messages"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "      Word  Frequency\n0       im        121\n1  support         87\n2     move         84\n3      get         83\n4    think         78\n5    would         73\n6     dont         72\n7    going         67\n8     like         60\n9     want         58",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Word</th>\n      <th>Frequency</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>im</td>\n      <td>121</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>support</td>\n      <td>87</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>move</td>\n      <td>84</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>get</td>\n      <td>83</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>think</td>\n      <td>78</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>would</td>\n      <td>73</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>dont</td>\n      <td>72</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>going</td>\n      <td>67</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>like</td>\n      <td>60</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>want</td>\n      <td>58</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# first quartile vocabulary distribution\n",
    "\n",
    "top_N = 10\n",
    "txt = df.sort_values('politeness').iloc[int(len(df)/4):,:].head(1000).messages.str.lower().str.replace(r'\\|', ' ').str.cat(sep=' ')\n",
    "txt = txt.translate(str.maketrans('', '', string.punctuation+'â€™Â´'))\n",
    "words = nltk.tokenize.word_tokenize(txt)\n",
    "\n",
    "words_dist = nltk.FreqDist(w for w in words if w not in stopwords+['england','russia','france','turkey','germany','italy','austria-hungary', 'austria'])\n",
    "\n",
    "rslt = pd.DataFrame(words_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "display(rslt)"
   ]
  },
  {
   "source": [
    "* #### beginning of last quartile vocabulary"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "      Word  Frequency\n0       im        227\n1    would        177\n2    think        174\n3     like        154\n4      get        125\n5  support        123\n6     well        118\n7     move         96\n8     yeah         88\n9      ill         88",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Word</th>\n      <th>Frequency</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>im</td>\n      <td>227</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>would</td>\n      <td>177</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>think</td>\n      <td>174</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>like</td>\n      <td>154</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>get</td>\n      <td>125</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>support</td>\n      <td>123</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>well</td>\n      <td>118</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>move</td>\n      <td>96</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>yeah</td>\n      <td>88</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>ill</td>\n      <td>88</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "#third quartile\n",
    "top_N = 10\n",
    "txt = df.sort_values('politeness').iloc[int(3*len(df)/4):,:].head(1000).messages.str.lower().str.replace(r'\\|', ' ').str.cat(sep=' ')\n",
    "txt = txt.translate(str.maketrans('', '', string.punctuation+'â€™Â´'))\n",
    "words = nltk.tokenize.word_tokenize(txt)\n",
    "\n",
    "words_dist = nltk.FreqDist(w for w in words if w not in stopwords+['england','russia','france','turkey','germany','italy','austria-hungary', 'austria'])\n",
    "\n",
    "rslt = pd.DataFrame(words_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "display(rslt)\n",
    "\n"
   ]
  },
  {
   "source": [
    "* #### most polite messages vocabulary"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "we take only 100 messages here to really be able to study extremely polite messages"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "      Word  Frequency\n0   thanks         48\n1    thank         43\n2     good         11\n3     much         10\n4     know         10\n5      yes         10\n6    great          8\n7     help          8\n8  forward          7\n9     okay          7",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Word</th>\n      <th>Frequency</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>thanks</td>\n      <td>48</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>thank</td>\n      <td>43</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>good</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>much</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>know</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>yes</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>great</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>help</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>forward</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>okay</td>\n      <td>7</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# Check for most polite vocabulary\n",
    "txt = df.sort_values('politeness', ascending=False).head(100).messages.str.lower().str.replace(r'\\|', ' ').str.cat(sep=' ')\n",
    "txt = txt.translate(str.maketrans('', '', string.punctuation+'â€™Â´'))\n",
    "words = nltk.tokenize.word_tokenize(txt)\n",
    "\n",
    "\n",
    "words_dist = nltk.FreqDist(w for w in words if w not in stopwords+['england','russia','france','turkey','germany','italy','austria-hungary', 'austria'])\n",
    "\n",
    "rslt = pd.DataFrame(words_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "display(rslt)"
   ]
  },
  {
   "source": [
    "### 3. doing the same analysis over VADER compond sentiments"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "* #### Looking at most negative messages"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Most negative messages (according to VADER)\nScore: -0.973 | Message: You're not, but you won't later either.  You can still hold Moscow with your fleet plus my army, and worst comes to worst, your army would be loose behind their front.  The alternative is we continue to attack you and we all lost out.  Italy, me, you\nScore: -0.972 | Message: And I hate to be so greedy to open up but I'm nervous that if I forego a build it opens me up to have Russia, France or both collapse on me and then we're really fucked. Particularly if France walks into Munich, I'll need both builds badly\nScore: -0.959 | Message: It depends: if they say \"I don't trust Italy, I'm ok not going to Gal if you aren't\", then you do blow some trust when you go to Gal.  BUT at the same time you went to Bla!  Austria and I commiserate around how much a dirty rat you are, you tell them that you were sure they were lying, and \"sorry I didn't trust you, but I'll keep attacking Turkey, watch, they think I'll park my fleet in Rum, but I'll attack them instead!\".  They could fall for it, as the moves look pretty weird at first.\nScore: -0.957 | Message: Itâ€™ll still probably end up that way, frankly.   Just seeing England roll over like a sick jellyfish and France continue to hold me off while crumbling to you... I figured worst case scenario: you still kill the entire west, I rule the east.  You realize youâ€™re *still* closer to Gibraltar than I am!  So yeah, I didnâ€™t think you were *planning* to screw me, I just looked at the map and saw: what if England and France just *gave him* the win?  Canâ€™t I just cover my ass so even if he grows by 4 next year, Iâ€™m still ok?  And seriously: watch how it goes: even with this move, there is nothing I or they can do to stop you from crushing that whole side of the board.  And in 3 full game years? Theyâ€™ll *still* both be dead!\nScore: -0.953 | Message: yeah that's an unpleasant surprise. Has Italy promised you anything? Based on some things I've pieced together, I'm starting to realize the full extent of their manipulation of this entire game. It's honestly kind of unsettling.\n\n...I hope this can stay between us, France, but I've got two sandboxes up right now: one where I put aside my beef with England just to try and get Italy under control, and one where I keep dancing with the devil, which I will follow through with only on the condition they agree *not* to take one of Liverpool or Portugal this year. Neither of these plans has you staying in Brest. If you're willing to do either of those things (help with an Italy stab or more English destruction) at the expense of Brest, let me know and let me know which you'd prefer. If not, well, that may tip the scales to me staying with Italy, because I don't think we can swing the stab without your help.\n"
     ]
    }
   ],
   "source": [
    "print('Most negative messages (according to VADER)')\n",
    "for _, row in df.sort_values('vader_score').head(5).iterrows():\n",
    "    print(f\"Score: {row['vader_score']:.3f} | Message: {row['messages']}\")"
   ]
  },
  {
   "source": [
    "* #### Looking at first sentiment (quite negative) quartile messages"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "First quartile messages sample sorted by VADER sentiments\nScore: 0.000 | Message: https://tenor.com/view/turkey-slice-cut-thanksgiving-gif-3575755\nScore: 0.000 | Message: Understandable\nScore: 0.000 | Message: I think I have something worked out that takes some of the issue off of you. You'll know it worked if you see some chairs in the West this turn. ðŸ˜€\nScore: 0.000 | Message: In my view, the fleet going to SC of Spain is a concession to England, but apparently he doesn't see things the same way. I suggest that we demilitarize Western Mediterranean for now. My fleet will go on standby in Lyon to defend Marseilles\nScore: 0.000 | Message: Are you getting much from Austria?\n"
     ]
    }
   ],
   "source": [
    "print('First quartile messages sample sorted by VADER sentiments')\n",
    "for _, row in df.sort_values('vader_score').iloc[int(len(df)/4):,:].head(5).iterrows():\n",
    "    print(f\"Score: {row['vader_score']:.3f} | Message: {row['messages']}\")"
   ]
  },
  {
   "source": [
    "* #### Looking at third sentiment (quite positive) quartile messages"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Third quartile messages sample sorted by VADER sentiments\nScore: 0.511 | Message: We have trust\nScore: 0.511 | Message: The Sev disband probably helped with the mutual trust ðŸ™‚\nScore: 0.511 | Message: Iâ€™m with you 110%- the hun has grown strong\nScore: 0.511 | Message: France is always strong early\nScore: 0.511 | Message: That seems the most likely option, I think it's a good idea to do that. The French have the opportunity to ruin you completely if they manage to secure both.\n"
     ]
    }
   ],
   "source": [
    "print('Third quartile messages sample sorted by VADER sentiments')\n",
    "for _, row in df.sort_values('vader_score').iloc[int(3*len(df)/4):,:].head(5).iterrows():\n",
    "    print(f\"Score: {row['vader_score']:.3f} | Message: {row['messages']}\")"
   ]
  },
  {
   "source": [
    "* #### Looking at most positive messages"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Most positive messages (according to VADER)\n",
      "Score: 0.992 | Message: I agree, and I think we can safely consider France as doomed. I'm not saying England isn't sincere in that goal as well, but I'm trying to look at next year's problems. Or the year after that. When France is down and we're looking at what to do about Turkey - or England as the case may be. \n",
      "\n",
      "Probably if I sat on Portugal/Spain/MAO I could quite comfortably work with England, but that puts him into the position of going through you or northern Russia. Since you haven't tried to play me yet (or you're doing so well I haven't noticed yet lol), I know who I'll pick if I'm forced to.\n",
      "\n",
      "Thanks! Yes I'm better now, just headachey. That sounds amazing, and I love the overnight together with friends. It sounds like so much fun, and a chance to catch up with her tribe before she marries into a new one :3\n",
      "Score: 0.986 | Message: I just messaged Russia. I told them about your suggestion of a three-way alliance, and that I was mainly focussed on attacking the west this year but if needed I could readjust to spare somebody to support in the east.\n",
      "\n",
      "I haven't heard back from France yet, but I am sort of playing double-agent with Italy to ensure that France can continue countering Italy's Spa/Mar plays and it just looks like luck. This keeps Italy in check until you and I make progress in the north and are better positioned to push Italy back.\n",
      "\n",
      "I should have mentioned, but a third reason for the fleet build last turn was to keep France's trust. I figured it made sense anyway for the other two reasons I mentioned, but I did want to make sure France is willing to listen to me about Italy.\n",
      "Score: 0.985 | Message: To my lovely and Newly wed \"Queen Bianca Di Medici\"\n",
      "Respectfully from yours truly:  (I, King Shetan personally articulate (at least try to) *slips into vaq mode slightly* this  weird kind of love letter to \"Esther Second Queen of England and Ruler of the Italians with all their subordinates.\") I hereby request  that my beautiful foreign Queens would lovingly allow their benevolent \"King Shetan The 1st\" to close out his empire at 12 scs and in Kingly fashion  generously bequeathing 11 scs to each Queen. (Not sure if that ends with a question mark or an exclamation point haha) whew. I'm glad I'm not actually a king. Would be exhausting to always \"try\" to speak with eloquent fluidity\n",
      "Score: 0.984 | Message: I mean, being honest is great, but I need to see honesty and moves that are mutually beneficial. What you did was honest, but not mutually beneficial. EC would be mutually beneficial, now we will see if you are honest.\n",
      "Score: 0.984 | Message: Oh but youâ€™re *not*! You agreed to warn me of unexpected moves, then didnâ€™t. When I brought this up you ignored it and misdirected me in hopes Iâ€™d forget. Youâ€™ve revealed things to England without my permission, and then made up a story about it after the fact!\n",
      "\n",
      "And you canâ€™t be a real partner with someone who is depending on your good graces to survive. Thatâ€™s not a partnership. We could never be real partners unless we had some notion of equality, and Iâ€™m outmatched in both skill and numbers.\n",
      "\n",
      "You and Austria, however, were until recently a perfect example of a true partnership. Dot-parity, coordinated attacks, really beautiful work. So donâ€™t act as if you donâ€™t know this to be true. We were never a partnership of that kind.\n"
     ]
    }
   ],
   "source": [
    "print('Most positive messages (according to VADER)')\n",
    "for _, row in df.sort_values('vader_score', ascending=False).head(5).iterrows():\n",
    "    print(f\"Score: {row['vader_score']:.3f} | Message: {row['messages']}\")"
   ]
  },
  {
   "source": [
    "### 4. Study of the vocabulary distribution"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "* #### most negative vocabulary distribution"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "         Word  Frequency\n0          im         40\n1         get         30\n2        stab         28\n3       would         26\n4      attack         25\n5       fleet         20\n6       could         20\n7        dont         19\n8         two         19\n9         war         17\n10  attacking         16\n11       game         16\n12         go         15\n13      think         15\n14       like         15",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Word</th>\n      <th>Frequency</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>im</td>\n      <td>40</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>get</td>\n      <td>30</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>stab</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>would</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>attack</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>fleet</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>could</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>dont</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>two</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>war</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>attacking</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>game</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>go</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>think</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>like</td>\n      <td>15</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "top_N = 15\n",
    "txt = df.sort_values('vader_score').head(100).messages.str.lower().str.replace(r'\\|', ' ').str.cat(sep=' ')\n",
    "txt = txt.translate(str.maketrans('', '', string.punctuation+'â€™Â´'))\n",
    "words = nltk.tokenize.word_tokenize(txt)\n",
    "\n",
    "words_dist = nltk.FreqDist(w for w in words if w not in stopwords+['england','russia','france','turkey','germany','italy','austria-hungary', 'austria'])\n",
    "\n",
    "rslt = pd.DataFrame(words_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "display(rslt)"
   ]
  },
  {
   "source": [
    "* #### quite negative vocabulary distribution (beginning of first quartile)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "    Word  Frequency\n0  think         76\n1      f         74\n2     im         68\n3  going         56\n4   move         49\n5    get         42\n6    ill         40\n7  would         39\n8   take         39\n9   know         38",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Word</th>\n      <th>Frequency</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>think</td>\n      <td>76</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>f</td>\n      <td>74</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>im</td>\n      <td>68</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>going</td>\n      <td>56</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>move</td>\n      <td>49</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>get</td>\n      <td>42</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>ill</td>\n      <td>40</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>would</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>take</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>know</td>\n      <td>38</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "top_N = 10\n",
    "txt = df.sort_values('vader_score').iloc[int(len(df)/4):,:].head(1000).messages.str.lower().str.replace(r'\\|', ' ').str.cat(sep=' ')\n",
    "txt = txt.translate(str.maketrans('', '', string.punctuation+'â€™Â´'))\n",
    "words = nltk.tokenize.word_tokenize(txt)\n",
    "\n",
    "words_dist = nltk.FreqDist(w for w in words if w not in stopwords+['england','russia','france','turkey','germany','italy','austria-hungary', 'austria'])\n",
    "\n",
    "rslt = pd.DataFrame(words_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "display(rslt)"
   ]
  },
  {
   "source": [
    "* #### quite positive vocabulary distribution (beginning of third quartile)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "      Word  Frequency\n0       im        195\n1    would        129\n2     like        129\n3  support        116\n4      get        113\n5     well        107\n6    think         97\n7    going         95\n8     move         94\n9     want         86",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Word</th>\n      <th>Frequency</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>im</td>\n      <td>195</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>would</td>\n      <td>129</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>like</td>\n      <td>129</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>support</td>\n      <td>116</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>get</td>\n      <td>113</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>well</td>\n      <td>107</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>think</td>\n      <td>97</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>going</td>\n      <td>95</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>move</td>\n      <td>94</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>want</td>\n      <td>86</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "txt = df.sort_values('vader_score').iloc[int(3*len(df)/4):,:].head(1000).messages.str.lower().str.replace(r'\\|', ' ').str.cat(sep=' ')\n",
    "txt = txt.translate(str.maketrans('', '', string.punctuation+'â€™Â´'))\n",
    "words = nltk.tokenize.word_tokenize(txt)\n",
    "\n",
    "words_dist = nltk.FreqDist(w for w in words if w not in stopwords+['england','russia','france','turkey','germany','italy','austria-hungary', 'austria'])\n",
    "\n",
    "rslt = pd.DataFrame(words_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "display(rslt)"
   ]
  },
  {
   "source": [
    "* #### very positive vocabulary distribution "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "          Word  Frequency\n0           im         69\n1         game         43\n2        would         41\n3          get         37\n4         good         36\n5         like         35\n6        think         34\n7          one         34\n8  wintergreen         34\n9      support         33",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Word</th>\n      <th>Frequency</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>im</td>\n      <td>69</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>game</td>\n      <td>43</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>would</td>\n      <td>41</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>get</td>\n      <td>37</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>good</td>\n      <td>36</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>like</td>\n      <td>35</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>think</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>one</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>wintergreen</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>support</td>\n      <td>33</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "txt = df.sort_values('vader_score', ascending=False).head(100).messages.str.lower().str.replace(r'\\|', ' ').str.cat(sep=' ')\n",
    "txt = txt.translate(str.maketrans('', '', string.punctuation+'â€™Â´'))\n",
    "\n",
    "# create a pattern that will match any country in pycountry.countries\n",
    "\n",
    "\n",
    "# apply sub for each company\n",
    "txt = pattern.sub(\"\", txt).strip()\n",
    "\n",
    "words = nltk.tokenize.word_tokenize(txt)\n",
    "\n",
    "words_dist = nltk.FreqDist(w for w in words if w not in stopwords+['england','russia','france','turkey','germany','italy','austria-hungary', 'austria'])\n",
    "\n",
    "rslt = pd.DataFrame(words_dist.most_common(top_N),\n",
    "                    columns=['Word', 'Frequency'])\n",
    "display(rslt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}